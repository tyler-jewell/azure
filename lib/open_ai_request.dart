/// OpenAI request.
///
/// https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#request-body
class OpenAIRequest {
  /// Creates a new `OpenAIRequest` instance.
  OpenAIRequest({
    this.prompt,
    this.maxTokens,
    this.temperature,
    this.topP,
    this.logitBias,
    this.user,
    this.n,
    this.stream,
    this.logprobs,
    this.suffix,
    this.echo,
    this.stop,
    this.completionConfig,
    this.presencePenalty,
    this.frequencyPenalty,
    this.bestOf,
  });

  /// The prompt(s) to generate completions for, encoded as a string or
  /// array of strings.
  ///
  /// Note that <|endoftext|> is the document separator that the model
  /// sees during training, so if a prompt isn'tspecified the model will
  /// generate as if from the beginning of a new document. Maximum allowed size
  /// of string list is 2048.
  final String? prompt;

  /// The token count of your prompt plus max_tokens can't exceed the model's
  /// context length. Most models have a context length of
  /// 2048 tokens (except for the newest models, which support 4096).
  ///
  /// Has minimum of 0.
  ///
  /// Defaults to 16
  final int? maxTokens;

  /// What sampling temperature to use. Higher values means the model will take
  /// more risks. Try 0.9 for more creative applications, and 0
  /// (arg max sampling)
  ///
  /// for ones with a well-defined answer.
  ///
  /// We generally recommend altering this or top_p but not both.
  ///
  /// Defaults to 1.0
  final double? temperature;

  /// An alternative to sampling with temperature, called nucleus sampling,
  /// where the model considers the results of the tokens with top_p probability
  /// mass. So 0.1 means only the tokens comprising the top 10% probability
  /// mass are considered.
  ///
  /// We generally recommend altering this or temperature but not both.
  final double? topP;

  /// Defaults to null. Modify the likelihood of specified tokens appearing in
  /// the completion. Accepts a json object that maps tokens
  /// (specified by their token ID in the GPT tokenizer) to an associated bias
  /// value from -100 to 100.
  ///
  /// You can use this tokenizer tool (which works for both GPT-2 and GPT-3) to
  /// convert text to token IDs. Mathematically, the bias is added to the logits
  /// generated by the model prior to sampling. The exact effect wi
  /// model, but values between -1 and 1 should decrease or increase likelihood
  /// of selection; values like -100 or 100 should result in a ban or exclusive
  /// selection of the relevant token. As an example, you can pass
  /// {"50256":-100} to prevent the <|endoftext|> token from being generated.
  final Map<String, int>? logitBias;

  /// A unique identifier representing your end-user, which can help monitoring
  /// and detecting abuse
  final String? user;

  /// How many completions to generate for each prompt. Minimum of 1 and
  /// maximum of 128 allowed.
  /// Note: Because this parameter generates many completions,
  /// it can quickly consume your token quota. Use carefully and ensure
  /// that you have reasonable settings for max_tokens and stop.
  final int? n;

  /// Whether to stream back partial progress. If set, tokens will be sent
  /// as data-only server-sent events as they become available, with the stream
  /// terminated by a data: `DONE` message.
  final bool? stream;

  /// Include the log probabilities on the logprobs most likely tokens, as well
  /// the chosen tokens. For example, if logprobs is 5, the API will return
  /// a list of the 5 most likely tokens. The API will always return the
  /// logprob of the sampled token, so there may be up to logprobs+1
  /// elements in the response.
  ///
  /// Minimum of 0 and maximum of 5 allowed.
  final int? logprobs;

  /// The suffix that comes after a completion of inserted text.
  final String? suffix;

  /// Echo back the prompt in addition to the completion
  final bool? echo;

  /// Up to 4 sequences where the API will stop generating further tokens.
  /// The returned text will not contain the stop sequence.
  final List<String>? stop;

  /// Completion config.
  final String? completionConfig;

  /// Number between -2.0 and 2.0. Positive values penalize new tokens
  /// based on whether they appear in the text so far, increasing the model's
  /// likelihood to talk about new topics.
  final double? presencePenalty;

  /// Number between -2.0 and 2.0. Positive values penalize new tokens based
  /// on their existing frequency in the text so far, decreasing the model's
  /// likelihood to repeat the same line verbatim.
  final double? frequencyPenalty;

  /// Generates best_of completions server-side and returns the "best"
  /// (defined as the one with the highest log probability per token).
  /// Results can't be streamed.
  ///
  /// When used with n, best_of controls the number of candidate completions
  /// and n specifies how many to return - best_of must be greater than n.
  ///
  /// Note: Because this parameter generates many completions, it can quickly
  /// consume your token quota. Use carefully and ensure that you have
  /// reasonable settings for max_tokens and stop. Has maximum value of 128.
  final int? bestOf;

  /// To json method.
  Map<String, dynamic> toJson() {
    return {
      if (prompt != null) 'prompt': prompt,
      if (maxTokens != null) 'max_tokens': maxTokens,
      if (temperature != null) 'temperature': temperature,
      if (topP != null) 'top_p': topP,
      if (logitBias != null) 'logit_bias': logitBias,
      if (user != null) 'user': user,
      if (n != null) 'n': n,
      if (stream != null) 'stream': stream,
      if (logprobs != null) 'logprobs': logprobs,
      if (suffix != null) 'suffix': suffix,
      if (echo != null) 'echo': echo,
      if (stop != null) 'stop': stop,
      if (completionConfig != null) 'completion_config': completionConfig,
      if (presencePenalty != null) 'presence_penalty': presencePenalty,
      if (frequencyPenalty != null) 'frequency_penalty': frequencyPenalty,
      if (bestOf != null) 'best_of': bestOf,
    };
  }
}
